# Complexity as Proof

Complexity is often taught and discussed within computer science as a method of
evaluating the performance of algorithms, data structures, and more generally
programs. Undergrads and competitive programmers learn about the scaling
resource usages of common algorithms, how they compare to one another, and how
to determine the bounds of new algorithms, and once they can pass the
software engineering interview the work is considered done.

A notion that is less commonly explored is the fact that complexity is a
fundamental mathematical property of the objects being referred to. This means
that by reasoning about the complexity of an algorithm or string, we can use
this information to prove further properties about its behaviour.

### Complexity of Integers

Consider positive integers. As they increase in size, by how much does
the spatial complexity of the number increase? For base 10, all numbers lower
than 10 require a single digit, numbers lower than 100 require 2, etc. This
means we can take the log base 10 of the actual value and know how many digits
required to represent that value in base 10. This applies across
other bases, and we can say that the spatial complexity for an integer n is on the
order O(log(n)).

Importantly, this is specifically the _upper_ bound for the representation of
an integer - there are some exceptions which we can
represent in a much more compressed form. Powers of 2 can all be represented as
the exponent of 2 that they are, so that 4294967296 can be represented as 2 ^
32, requiring a complexity of log(log2(n)).

TODO Show that the below is true by using base 2, in the same way as MIU system

It
should be clear that the upper bound for the general case cannot be
lower than O(log(n)), even though we can apply our compression to a subset of
the integers. There will always be numbers that require the full upper bound
in order to be represented, and where we need to be at least as specific as the
base 10 representation.

### Complexity-based Proof for Infinite Primes

- Fundamental shift from thinking about complexity as a way of analysing a data
structure or function to thinking about it as a property we can leverage
- In other words, if we can find two representations that are equivalent (can
represent the same set), we can do more than compare, we can make statements

- Let's expand our power of 2 example to not just 2 multiplied to itself, but
primes multiplied together (this is prime factorisation)
- Has the same form as power of 2, but with many primes
- But this time we can represent all the value we can using a number base
- Therefore, our representation as primes must also have an upper bound of O(log(n))

- Many beautiful proofs that there are infinite primes
- Chaitin showed you can use complexity to make statements about this too
- Consider if there were only finitely many primes - once our numbers get large
enough, and we run out of new primes to multiply, our prime factors
representation starts growing much more slowly, even though our digits
representation is still increasing at the same rate
- Saying there are finitely many primes is akin to saying the only prime is 2 -
with O(k.log(log(n))) with k as the number of primes.

The reason I like this proof is because it presents a deeper way of thinking
about complexity, not just as a way of comparing the expected cost of a data
structure of algorithm in computation, but as a powerful property we can use
to formally reason about things in a familiar setting.

### Further Reading

Metamaths
